=== LLaDA 后端服务日志 ===
启动时间: Mon Jul 14 01:00:08 CST 2025
日志会话: 20250714_010008
Python版本: Python 3.12.3
工作目录: /root/LLaDA-main/lldm
设备检测: CUDA可用: True
==========================

2025-07-14 01:00:15 - INFO - [server.py:102] - 初始化设备: cuda
2025-07-14 01:00:15 - INFO - [server.py:109] - GPU总内存: 23.5GB
2025-07-14 01:00:15 - INFO - [server.py:112] - 开始加载tokenizer...
2025-07-14 01:00:15 - INFO - [server.py:114] - Tokenizer加载成功
2025-07-14 01:00:15 - INFO - [server.py:116] - 开始加载模型...
2025-07-14 01:00:16 - INFO - [modeling.py:991] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-07-14 01:00:16 - WARNING - [modeling.py:1114] - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
2025-07-14 01:00:21 - INFO - [server.py:134] - 模型加载成功，设备: cuda
2025-07-14 01:00:21 - INFO - [server.py:137] - GPU已分配内存: 14.9GB
2025-07-14 01:00:21 - INFO - [server.py:512] - ==================================================
2025-07-14 01:00:21 - INFO - [server.py:513] - 🚀 LLaDA后端服务启动
2025-07-14 01:00:21 - INFO - [server.py:514] - 启动时间: 2025-07-14T01:00:21.099556
2025-07-14 01:00:21 - INFO - [server.py:515] - Python版本: 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:46:43) [GCC 11.2.0]
2025-07-14 01:00:21 - INFO - [server.py:516] - 工作目录: /root/LLaDA-main/lldm
2025-07-14 01:00:21 - INFO - [server.py:517] - 设备: cuda
2025-07-14 01:00:21 - INFO - [server.py:519] - GPU内存: 23.5GB
2025-07-14 01:00:21 - INFO - [server.py:520] - ==================================================
2025-07-14 01:00:21 - INFO - [_internal.py:97] - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:9000
 * Running on http://172.17.0.3:9000
2025-07-14 01:00:21 - INFO - [_internal.py:97] - [33mPress CTRL+C to quit[0m
2025-07-14 01:00:22 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "curl/7.81.0",
  "timestamp": "2025-07-14T01:00:22.072680"
}
2025-07-14 01:00:22 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:00:22.073835",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:00:22 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:00:22] "GET /health HTTP/1.1" 200 -
2025-07-14 01:00:29 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "python-requests/2.31.0",
  "timestamp": "2025-07-14T01:00:29.646283"
}
2025-07-14 01:00:29 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:00:29.646883",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:00:29 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:00:29] "GET /health HTTP/1.1" 200 -
2025-07-14 01:00:30 - INFO - [server.py:78] - 收到请求: {
  "method": "POST",
  "url": "http://localhost:9000/generate",
  "remote_addr": "127.0.0.1",
  "user_agent": "python-requests/2.31.0",
  "timestamp": "2025-07-14T01:00:30.653254",
  "json_data": {
    "messages": [
      {
        "role": "user",
        "content": "你好"
      }
    ],
    "settings": {
      "gen_length": 16,
      "steps": 8,
      "temperature": 0.0,
      "cfg_scale": 0.0
    }
  }
}
2025-07-14 01:00:30 - INFO - [server.py:448] - [req_1752426030654] 收到生成请求
2025-07-14 01:00:30 - INFO - [server.py:456] - [req_1752426030654] 请求数据详情:
2025-07-14 01:00:30 - INFO - [server.py:457] - [req_1752426030654] - 消息数量: 1
2025-07-14 01:00:30 - INFO - [server.py:458] - [req_1752426030654] - 设置参数: {
  "gen_length": 16,
  "steps": 8,
  "temperature": 0.0,
  "cfg_scale": 0.0
}
2025-07-14 01:00:30 - DEBUG - [server.py:459] - [req_1752426030654] - 完整消息列表: [
  {
    "role": "user",
    "content": "你好"
  }
]
2025-07-14 01:00:30 - INFO - [server.py:470] - [req_1752426030654] 开始生成响应...
2025-07-14 01:00:30 - INFO - [server.py:206] - ============================================================
2025-07-14 01:00:30 - INFO - [server.py:207] - 开始生成响应
2025-07-14 01:00:30 - INFO - [server.py:208] - 输入消息数量: 1
2025-07-14 01:00:30 - INFO - [server.py:210] - 消息 1: 角色=user, 内容长度=2
2025-07-14 01:00:30 - DEBUG - [server.py:211] - 消息 1 内容: 你好
2025-07-14 01:00:30 - INFO - [server.py:213] - 设置参数: {
  "gen_length": 16,
  "steps": 8,
  "temperature": 0.0,
  "cfg_scale": 0.0
}
2025-07-14 01:00:30 - INFO - [server.py:225] - 解析后的约束: {}
2025-07-14 01:00:30 - INFO - [server.py:230] - 生成前GPU内存使用: 14.93GB
2025-07-14 01:00:30 - INFO - [server.py:233] - 准备输入prompt...
2025-07-14 01:00:30 - DEBUG - [server.py:235] - 格式化后的chat输入: <|startoftext|><|start_header_id|>user<|end_header_id|>

你好<|eot_id|><|start_header_id|>assistant<|end_header_id|>


2025-07-14 01:00:30 - INFO - [server.py:239] - 输入token数量: 14
2025-07-14 01:00:30 - INFO - [server.py:246] - 序列总长度: 30, prompt长度: 14, 生成长度: 16
2025-07-14 01:00:30 - INFO - [server.py:255] - 应用约束数量: 0
2025-07-14 01:00:30 - INFO - [server.py:275] - 块处理配置: 块长度=16, 块数量=1, 每块步数=8
2025-07-14 01:00:30 - INFO - [server.py:279] - 处理第 1/1 块...
2025-07-14 01:00:30 - DEBUG - [server.py:294] - 块 1, 步骤 1/8
2025-07-14 01:00:31 - DEBUG - [server.py:294] - 块 1, 步骤 2/8
2025-07-14 01:00:31 - DEBUG - [server.py:294] - 块 1, 步骤 3/8
2025-07-14 01:00:31 - DEBUG - [server.py:294] - 块 1, 步骤 4/8
2025-07-14 01:00:31 - DEBUG - [server.py:294] - 块 1, 步骤 5/8
2025-07-14 01:00:31 - DEBUG - [server.py:294] - 块 1, 步骤 6/8
2025-07-14 01:00:31 - DEBUG - [server.py:294] - 块 1, 步骤 7/8
2025-07-14 01:00:31 - DEBUG - [server.py:294] - 块 1, 步骤 8/8
2025-07-14 01:00:31 - INFO - [server.py:400] - 生成后GPU内存使用: 15.36GB
2025-07-14 01:00:31 - INFO - [server.py:402] - 生成完成!
2025-07-14 01:00:31 - INFO - [server.py:403] - 响应文本长度: 17
2025-07-14 01:00:31 - INFO - [server.py:404] - 可视化步骤数: 9
2025-07-14 01:00:31 - INFO - [server.py:405] - 生成耗时: 0.75秒
2025-07-14 01:00:31 - DEBUG - [server.py:406] - 生成的响应文本: 你好！有什么我可以可以帮助你的吗？
2025-07-14 01:00:31 - INFO - [server.py:407] - ============================================================
2025-07-14 01:00:31 - INFO - [server.py:479] - [req_1752426030654] 生成成功!
2025-07-14 01:00:31 - INFO - [server.py:480] - [req_1752426030654] 响应长度: 17
2025-07-14 01:00:31 - INFO - [server.py:481] - [req_1752426030654] 可视化步骤: 9
2025-07-14 01:00:31 - INFO - [server.py:482] - [req_1752426030654] 总耗时: 0.76秒
2025-07-14 01:00:31 - DEBUG - [server.py:483] - [req_1752426030654] 响应文本: 你好！有什么我可以可以帮助你的吗？
2025-07-14 01:00:31 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 3111,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:00:31.409738"
}
2025-07-14 01:00:31 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:00:31] "POST /generate HTTP/1.1" 200 -
2025-07-14 01:00:32 - INFO - [server.py:78] - 收到请求: {
  "method": "POST",
  "url": "http://localhost:9000/generate",
  "remote_addr": "127.0.0.1",
  "user_agent": "python-requests/2.31.0",
  "timestamp": "2025-07-14T01:00:32.413667",
  "json_data": {
    "messages": [
      {
        "role": "user",
        "content": "今天天气怎么样？"
      }
    ],
    "settings": {
      "gen_length": 12,
      "steps": 6,
      "temperature": 0.0,
      "constraints": "0:今天, 2:天气, 4:很好"
    }
  }
}
2025-07-14 01:00:32 - INFO - [server.py:448] - [req_1752426032414] 收到生成请求
2025-07-14 01:00:32 - INFO - [server.py:456] - [req_1752426032414] 请求数据详情:
2025-07-14 01:00:32 - INFO - [server.py:457] - [req_1752426032414] - 消息数量: 1
2025-07-14 01:00:32 - INFO - [server.py:458] - [req_1752426032414] - 设置参数: {
  "gen_length": 12,
  "steps": 6,
  "temperature": 0.0,
  "constraints": "0:今天, 2:天气, 4:很好"
}
2025-07-14 01:00:32 - DEBUG - [server.py:459] - [req_1752426032414] - 完整消息列表: [
  {
    "role": "user",
    "content": "今天天气怎么样？"
  }
]
2025-07-14 01:00:32 - INFO - [server.py:470] - [req_1752426032414] 开始生成响应...
2025-07-14 01:00:32 - INFO - [server.py:206] - ============================================================
2025-07-14 01:00:32 - INFO - [server.py:207] - 开始生成响应
2025-07-14 01:00:32 - INFO - [server.py:208] - 输入消息数量: 1
2025-07-14 01:00:32 - INFO - [server.py:210] - 消息 1: 角色=user, 内容长度=8
2025-07-14 01:00:32 - DEBUG - [server.py:211] - 消息 1 内容: 今天天气怎么样？
2025-07-14 01:00:32 - INFO - [server.py:213] - 设置参数: {
  "gen_length": 12,
  "steps": 6,
  "temperature": 0.0,
  "constraints": "0:今天, 2:天气, 4:很好"
}
2025-07-14 01:00:32 - INFO - [server.py:225] - 解析后的约束: {0: '今天', 2: '天气', 4: '很好'}
2025-07-14 01:00:32 - INFO - [server.py:230] - 生成前GPU内存使用: 14.94GB
2025-07-14 01:00:32 - INFO - [server.py:233] - 准备输入prompt...
2025-07-14 01:00:32 - DEBUG - [server.py:235] - 格式化后的chat输入: <|startoftext|><|start_header_id|>user<|end_header_id|>

今天天气怎么样？<|eot_id|><|start_header_id|>assistant<|end_header_id|>


2025-07-14 01:00:32 - INFO - [server.py:239] - 输入token数量: 17
2025-07-14 01:00:32 - INFO - [server.py:246] - 序列总长度: 29, prompt长度: 17, 生成长度: 12
2025-07-14 01:00:32 - INFO - [server.py:255] - 应用约束数量: 5
2025-07-14 01:00:32 - DEBUG - [server.py:260] - 约束位置 0 (绝对位置 17): token_id=34808
2025-07-14 01:00:32 - DEBUG - [server.py:260] - 约束位置 2 (绝对位置 19): token_id=220
2025-07-14 01:00:32 - DEBUG - [server.py:260] - 约束位置 3 (绝对位置 20): token_id=13092
2025-07-14 01:00:32 - DEBUG - [server.py:260] - 约束位置 4 (绝对位置 21): token_id=220
2025-07-14 01:00:32 - DEBUG - [server.py:260] - 约束位置 5 (绝对位置 22): token_id=9052
2025-07-14 01:00:32 - INFO - [server.py:275] - 块处理配置: 块长度=12, 块数量=1, 每块步数=6
2025-07-14 01:00:32 - INFO - [server.py:279] - 处理第 1/1 块...
2025-07-14 01:00:32 - DEBUG - [server.py:294] - 块 1, 步骤 1/6
2025-07-14 01:00:32 - DEBUG - [server.py:294] - 块 1, 步骤 2/6
2025-07-14 01:00:32 - DEBUG - [server.py:294] - 块 1, 步骤 3/6
2025-07-14 01:00:32 - DEBUG - [server.py:294] - 块 1, 步骤 4/6
2025-07-14 01:00:32 - DEBUG - [server.py:294] - 块 1, 步骤 5/6
2025-07-14 01:00:32 - DEBUG - [server.py:294] - 块 1, 步骤 6/6
2025-07-14 01:00:32 - INFO - [server.py:400] - 生成后GPU内存使用: 15.35GB
2025-07-14 01:00:32 - INFO - [server.py:402] - 生成完成!
2025-07-14 01:00:32 - INFO - [server.py:403] - 响应文本长度: 22
2025-07-14 01:00:32 - INFO - [server.py:404] - 可视化步骤数: 7
2025-07-14 01:00:32 - INFO - [server.py:405] - 生成耗时: 0.31秒
2025-07-14 01:00:32 - DEBUG - [server.py:406] - 生成的响应文本:  今天 的 天气 很好，阳光明媚，气温适宜。
2025-07-14 01:00:32 - INFO - [server.py:407] - ============================================================
2025-07-14 01:00:32 - INFO - [server.py:479] - [req_1752426032414] 生成成功!
2025-07-14 01:00:32 - INFO - [server.py:480] - [req_1752426032414] 响应长度: 22
2025-07-14 01:00:32 - INFO - [server.py:481] - [req_1752426032414] 可视化步骤: 7
2025-07-14 01:00:32 - INFO - [server.py:482] - [req_1752426032414] 总耗时: 0.31秒
2025-07-14 01:00:32 - DEBUG - [server.py:483] - [req_1752426032414] 响应文本:  今天 的 天气 很好，阳光明媚，气温适宜。
2025-07-14 01:00:32 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 2096,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:00:32.726850"
}
2025-07-14 01:00:32 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:00:32] "POST /generate HTTP/1.1" 200 -
2025-07-14 01:00:34 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:00:34.163404"
}
2025-07-14 01:00:34 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:00:34.163984",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:00:34 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:00:34] "GET /health HTTP/1.1" 200 -
2025-07-14 01:00:34 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:00:34.265956"
}
2025-07-14 01:00:34 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:00:34.266696",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:00:34 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:00:34] "GET /health HTTP/1.1" 200 -
2025-07-14 01:00:36 - INFO - [server.py:78] - 收到请求: {
  "method": "OPTIONS",
  "url": "http://localhost:9000/generate",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:00:36.622821"
}
2025-07-14 01:00:36 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": null,
  "content_type": "text/html; charset=utf-8",
  "timestamp": "2025-07-14T01:00:36.623824"
}
2025-07-14 01:00:36 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:00:36] "OPTIONS /generate HTTP/1.1" 200 -
2025-07-14 01:00:36 - INFO - [server.py:78] - 收到请求: {
  "method": "POST",
  "url": "http://localhost:9000/generate",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:00:36.894021",
  "json_data": {
    "messages": [
      {
        "role": "user",
        "content": "你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。"
      }
    ],
    "settings": {
      "temperature": 0,
      "top_p": 0.95,
      "gen_length": 50,
      "num_beams": 4,
      "steps": 32,
      "cfg_scale": 1,
      "constraints": ""
    }
  }
}
2025-07-14 01:00:36 - INFO - [server.py:448] - [req_1752426036894] 收到生成请求
2025-07-14 01:00:36 - INFO - [server.py:456] - [req_1752426036894] 请求数据详情:
2025-07-14 01:00:36 - INFO - [server.py:457] - [req_1752426036894] - 消息数量: 1
2025-07-14 01:00:36 - INFO - [server.py:458] - [req_1752426036894] - 设置参数: {
  "temperature": 0,
  "top_p": 0.95,
  "gen_length": 50,
  "num_beams": 4,
  "steps": 32,
  "cfg_scale": 1,
  "constraints": ""
}
2025-07-14 01:00:36 - DEBUG - [server.py:459] - [req_1752426036894] - 完整消息列表: [
  {
    "role": "user",
    "content": "你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。"
  }
]
2025-07-14 01:00:36 - INFO - [server.py:470] - [req_1752426036894] 开始生成响应...
2025-07-14 01:00:36 - INFO - [server.py:206] - ============================================================
2025-07-14 01:00:36 - INFO - [server.py:207] - 开始生成响应
2025-07-14 01:00:36 - INFO - [server.py:208] - 输入消息数量: 1
2025-07-14 01:00:36 - INFO - [server.py:210] - 消息 1: 角色=user, 内容长度=98
2025-07-14 01:00:36 - DEBUG - [server.py:211] - 消息 1 内容: 你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。
2025-07-14 01:00:36 - INFO - [server.py:213] - 设置参数: {
  "temperature": 0,
  "top_p": 0.95,
  "gen_length": 50,
  "num_beams": 4,
  "steps": 32,
  "cfg_scale": 1,
  "constraints": ""
}
2025-07-14 01:00:36 - INFO - [server.py:225] - 解析后的约束: {}
2025-07-14 01:00:36 - INFO - [server.py:230] - 生成前GPU内存使用: 14.94GB
2025-07-14 01:00:36 - INFO - [server.py:233] - 准备输入prompt...
2025-07-14 01:00:36 - DEBUG - [server.py:235] - 格式化后的chat输入: <|startoftext|><|start_header_id|>user<|end_header_id|>

你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。<|eot_id|><|start_header_id|>assistant<|end_header_id|>


2025-07-14 01:00:36 - INFO - [server.py:239] - 输入token数量: 62
2025-07-14 01:00:36 - INFO - [server.py:246] - 序列总长度: 112, prompt长度: 62, 生成长度: 50
2025-07-14 01:00:36 - INFO - [server.py:255] - 应用约束数量: 0
2025-07-14 01:00:36 - INFO - [server.py:275] - 块处理配置: 块长度=32, 块数量=2, 每块步数=16
2025-07-14 01:00:36 - INFO - [server.py:279] - 处理第 1/2 块...
2025-07-14 01:00:36 - DEBUG - [server.py:294] - 块 1, 步骤 1/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 2/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 3/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 4/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 5/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 6/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 7/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 8/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 9/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 10/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 11/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 12/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 13/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 14/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 15/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 1, 步骤 16/16
2025-07-14 01:00:37 - INFO - [server.py:279] - 处理第 2/2 块...
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 2, 步骤 1/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 2, 步骤 2/16
2025-07-14 01:00:37 - DEBUG - [server.py:294] - 块 2, 步骤 3/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 4/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 5/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 6/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 7/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 8/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 9/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 10/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 11/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 12/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 13/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 14/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 15/16
2025-07-14 01:00:38 - DEBUG - [server.py:294] - 块 2, 步骤 16/16
2025-07-14 01:00:38 - INFO - [server.py:400] - 生成后GPU内存使用: 17.93GB
2025-07-14 01:00:38 - INFO - [server.py:402] - 生成完成!
2025-07-14 01:00:38 - INFO - [server.py:403] - 响应文本长度: 97
2025-07-14 01:00:38 - INFO - [server.py:404] - 可视化步骤数: 33
2025-07-14 01:00:38 - INFO - [server.py:405] - 生成耗时: 1.75秒
2025-07-14 01:00:38 - DEBUG - [server.py:406] - 生成的响应文本: 是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。
2025-07-14 01:00:38 - INFO - [server.py:407] - ============================================================
2025-07-14 01:00:38 - INFO - [server.py:479] - [req_1752426036894] 生成成功!
2025-07-14 01:00:38 - INFO - [server.py:480] - [req_1752426036894] 响应长度: 97
2025-07-14 01:00:38 - INFO - [server.py:481] - [req_1752426036894] 可视化步骤: 33
2025-07-14 01:00:38 - INFO - [server.py:482] - [req_1752426036894] 总耗时: 1.75秒
2025-07-14 01:00:38 - DEBUG - [server.py:483] - [req_1752426036894] 响应文本: 是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。
2025-07-14 01:00:38 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 39804,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:00:38.650270"
}
2025-07-14 01:00:38 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:00:38] "POST /generate HTTP/1.1" 200 -
2025-07-14 01:00:59 - INFO - [server.py:78] - 收到请求: {
  "method": "OPTIONS",
  "url": "http://localhost:9000/generate",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:00:59.865330"
}
2025-07-14 01:00:59 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": null,
  "content_type": "text/html; charset=utf-8",
  "timestamp": "2025-07-14T01:00:59.866433"
}
2025-07-14 01:00:59 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:00:59] "OPTIONS /generate HTTP/1.1" 200 -
2025-07-14 01:01:00 - INFO - [server.py:78] - 收到请求: {
  "method": "POST",
  "url": "http://localhost:9000/generate",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:01:00.301077",
  "json_data": {
    "messages": [
      {
        "role": "user",
        "content": "你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。"
      },
      {
        "role": "assistant",
        "content": "是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。"
      },
      {
        "role": "user",
        "content": "你是谁"
      }
    ],
    "settings": {
      "temperature": 0,
      "top_p": 0.95,
      "gen_length": 50,
      "num_beams": 4,
      "steps": 32,
      "cfg_scale": 1,
      "constraints": ""
    }
  }
}
2025-07-14 01:01:00 - INFO - [server.py:448] - [req_1752426060302] 收到生成请求
2025-07-14 01:01:00 - INFO - [server.py:456] - [req_1752426060302] 请求数据详情:
2025-07-14 01:01:00 - INFO - [server.py:457] - [req_1752426060302] - 消息数量: 3
2025-07-14 01:01:00 - INFO - [server.py:458] - [req_1752426060302] - 设置参数: {
  "temperature": 0,
  "top_p": 0.95,
  "gen_length": 50,
  "num_beams": 4,
  "steps": 32,
  "cfg_scale": 1,
  "constraints": ""
}
2025-07-14 01:01:00 - DEBUG - [server.py:459] - [req_1752426060302] - 完整消息列表: [
  {
    "role": "user",
    "content": "你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。"
  },
  {
    "role": "assistant",
    "content": "是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。"
  },
  {
    "role": "user",
    "content": "你是谁"
  }
]
2025-07-14 01:01:00 - INFO - [server.py:470] - [req_1752426060302] 开始生成响应...
2025-07-14 01:01:00 - INFO - [server.py:206] - ============================================================
2025-07-14 01:01:00 - INFO - [server.py:207] - 开始生成响应
2025-07-14 01:01:00 - INFO - [server.py:208] - 输入消息数量: 3
2025-07-14 01:01:00 - INFO - [server.py:210] - 消息 1: 角色=user, 内容长度=98
2025-07-14 01:01:00 - DEBUG - [server.py:211] - 消息 1 内容: 你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。
2025-07-14 01:01:00 - INFO - [server.py:210] - 消息 2: 角色=assistant, 内容长度=97
2025-07-14 01:01:00 - DEBUG - [server.py:211] - 消息 2 内容: 是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。
2025-07-14 01:01:00 - INFO - [server.py:210] - 消息 3: 角色=user, 内容长度=3
2025-07-14 01:01:00 - DEBUG - [server.py:211] - 消息 3 内容: 你是谁
2025-07-14 01:01:00 - INFO - [server.py:213] - 设置参数: {
  "temperature": 0,
  "top_p": 0.95,
  "gen_length": 50,
  "num_beams": 4,
  "steps": 32,
  "cfg_scale": 1,
  "constraints": ""
}
2025-07-14 01:01:00 - INFO - [server.py:225] - 解析后的约束: {}
2025-07-14 01:01:00 - INFO - [server.py:230] - 生成前GPU内存使用: 14.94GB
2025-07-14 01:01:00 - INFO - [server.py:233] - 准备输入prompt...
2025-07-14 01:01:00 - DEBUG - [server.py:235] - 格式化后的chat输入: <|startoftext|><|start_header_id|>user<|end_header_id|>

你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。<|eot_id|><|start_header_id|>assistant<|end_header_id|>

是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。<|eot_id|><|start_header_id|>user<|end_header_id|>

你是谁<|eot_id|><|start_header_id|>assistant<|end_header_id|>


2025-07-14 01:01:00 - INFO - [server.py:239] - 输入token数量: 123
2025-07-14 01:01:00 - INFO - [server.py:246] - 序列总长度: 173, prompt长度: 123, 生成长度: 50
2025-07-14 01:01:00 - INFO - [server.py:255] - 应用约束数量: 0
2025-07-14 01:01:00 - INFO - [server.py:275] - 块处理配置: 块长度=32, 块数量=2, 每块步数=16
2025-07-14 01:01:00 - INFO - [server.py:279] - 处理第 1/2 块...
2025-07-14 01:01:00 - DEBUG - [server.py:294] - 块 1, 步骤 1/16
2025-07-14 01:01:00 - DEBUG - [server.py:294] - 块 1, 步骤 2/16
2025-07-14 01:01:00 - DEBUG - [server.py:294] - 块 1, 步骤 3/16
2025-07-14 01:01:00 - DEBUG - [server.py:294] - 块 1, 步骤 4/16
2025-07-14 01:01:00 - DEBUG - [server.py:294] - 块 1, 步骤 5/16
2025-07-14 01:01:00 - DEBUG - [server.py:294] - 块 1, 步骤 6/16
2025-07-14 01:01:00 - DEBUG - [server.py:294] - 块 1, 步骤 7/16
2025-07-14 01:01:00 - DEBUG - [server.py:294] - 块 1, 步骤 8/16
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 1, 步骤 9/16
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 1, 步骤 10/16
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 1, 步骤 11/16
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 1, 步骤 12/16
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 1, 步骤 13/16
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 1, 步骤 14/16
2025-07-14 01:01:01 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:01:01.553774"
}
2025-07-14 01:01:01 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:01:01.554530",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:01:01 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:01:01] "GET /health HTTP/1.1" 200 -
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 1, 步骤 15/16
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 1, 步骤 16/16
2025-07-14 01:01:01 - INFO - [server.py:279] - 处理第 2/2 块...
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 2, 步骤 1/16
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 2, 步骤 2/16
2025-07-14 01:01:01 - DEBUG - [server.py:294] - 块 2, 步骤 3/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 4/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 5/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 6/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 7/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 8/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 9/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 10/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 11/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 12/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 13/16
2025-07-14 01:01:02 - DEBUG - [server.py:294] - 块 2, 步骤 14/16
2025-07-14 01:01:03 - DEBUG - [server.py:294] - 块 2, 步骤 15/16
2025-07-14 01:01:03 - DEBUG - [server.py:294] - 块 2, 步骤 16/16
2025-07-14 01:01:03 - INFO - [server.py:400] - 生成后GPU内存使用: 19.47GB
2025-07-14 01:01:03 - INFO - [server.py:402] - 生成完成!
2025-07-14 01:01:03 - INFO - [server.py:403] - 响应文本长度: 98
2025-07-14 01:01:03 - INFO - [server.py:404] - 可视化步骤数: 33
2025-07-14 01:01:03 - INFO - [server.py:405] - 生成耗时: 2.87秒
2025-07-14 01:01:03 - DEBUG - [server.py:406] - 生成的响应文本: 我是款名为LLaDA的由蚂蚁集团研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。
2025-07-14 01:01:03 - INFO - [server.py:407] - ============================================================
2025-07-14 01:01:03 - INFO - [server.py:479] - [req_1752426060302] 生成成功!
2025-07-14 01:01:03 - INFO - [server.py:480] - [req_1752426060302] 响应长度: 98
2025-07-14 01:01:03 - INFO - [server.py:481] - [req_1752426060302] 可视化步骤: 33
2025-07-14 01:01:03 - INFO - [server.py:482] - [req_1752426060302] 总耗时: 2.88秒
2025-07-14 01:01:03 - DEBUG - [server.py:483] - [req_1752426060302] 响应文本: 我是款名为LLaDA的由蚂蚁集团研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。
2025-07-14 01:01:03 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 40063,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:01:03.178186"
}
2025-07-14 01:01:03 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:01:03] "POST /generate HTTP/1.1" 200 -
2025-07-14 01:01:03 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:01:03.674983"
}
2025-07-14 01:01:03 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:01:03.675745",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:01:03 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:01:03] "GET /health HTTP/1.1" 200 -
2025-07-14 01:01:15 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:01:15.254274"
}
2025-07-14 01:01:15 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:01:15.255112",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:01:15 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:01:15] "GET /health HTTP/1.1" 200 -
2025-07-14 01:01:18 - INFO - [server.py:78] - 收到请求: {
  "method": "OPTIONS",
  "url": "http://localhost:9000/generate",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:01:18.564483"
}
2025-07-14 01:01:18 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": null,
  "content_type": "text/html; charset=utf-8",
  "timestamp": "2025-07-14T01:01:18.565484"
}
2025-07-14 01:01:18 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:01:18] "OPTIONS /generate HTTP/1.1" 200 -
2025-07-14 01:01:18 - INFO - [server.py:78] - 收到请求: {
  "method": "POST",
  "url": "http://localhost:9000/generate",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:01:18.792532",
  "json_data": {
    "messages": [
      {
        "role": "user",
        "content": "你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。"
      },
      {
        "role": "assistant",
        "content": "是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。"
      },
      {
        "role": "user",
        "content": "你是谁"
      },
      {
        "role": "assistant",
        "content": "我是款名为LLaDA的由蚂蚁集团研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。"
      },
      {
        "role": "user",
        "content": "我是谁"
      }
    ],
    "settings": {
      "temperature": 0,
      "top_p": 0.95,
      "gen_length": 50,
      "num_beams": 4,
      "steps": 32,
      "cfg_scale": 1,
      "constraints": ""
    }
  }
}
2025-07-14 01:01:18 - INFO - [server.py:448] - [req_1752426078793] 收到生成请求
2025-07-14 01:01:18 - INFO - [server.py:456] - [req_1752426078793] 请求数据详情:
2025-07-14 01:01:18 - INFO - [server.py:457] - [req_1752426078793] - 消息数量: 5
2025-07-14 01:01:18 - INFO - [server.py:458] - [req_1752426078793] - 设置参数: {
  "temperature": 0,
  "top_p": 0.95,
  "gen_length": 50,
  "num_beams": 4,
  "steps": 32,
  "cfg_scale": 1,
  "constraints": ""
}
2025-07-14 01:01:18 - DEBUG - [server.py:459] - [req_1752426078793] - 完整消息列表: [
  {
    "role": "user",
    "content": "你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。"
  },
  {
    "role": "assistant",
    "content": "是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。"
  },
  {
    "role": "user",
    "content": "你是谁"
  },
  {
    "role": "assistant",
    "content": "我是款名为LLaDA的由蚂蚁集团研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。"
  },
  {
    "role": "user",
    "content": "我是谁"
  }
]
2025-07-14 01:01:18 - INFO - [server.py:470] - [req_1752426078793] 开始生成响应...
2025-07-14 01:01:18 - INFO - [server.py:206] - ============================================================
2025-07-14 01:01:18 - INFO - [server.py:207] - 开始生成响应
2025-07-14 01:01:18 - INFO - [server.py:208] - 输入消息数量: 5
2025-07-14 01:01:18 - INFO - [server.py:210] - 消息 1: 角色=user, 内容长度=98
2025-07-14 01:01:18 - DEBUG - [server.py:211] - 消息 1 内容: 你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。
2025-07-14 01:01:18 - INFO - [server.py:210] - 消息 2: 角色=assistant, 内容长度=97
2025-07-14 01:01:18 - DEBUG - [server.py:211] - 消息 2 内容: 是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。
2025-07-14 01:01:18 - INFO - [server.py:210] - 消息 3: 角色=user, 内容长度=3
2025-07-14 01:01:18 - DEBUG - [server.py:211] - 消息 3 内容: 你是谁
2025-07-14 01:01:18 - INFO - [server.py:210] - 消息 4: 角色=assistant, 内容长度=98
2025-07-14 01:01:18 - DEBUG - [server.py:211] - 消息 4 内容: 我是款名为LLaDA的由蚂蚁集团研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。
2025-07-14 01:01:18 - INFO - [server.py:210] - 消息 5: 角色=user, 内容长度=3
2025-07-14 01:01:18 - DEBUG - [server.py:211] - 消息 5 内容: 我是谁
2025-07-14 01:01:18 - INFO - [server.py:213] - 设置参数: {
  "temperature": 0,
  "top_p": 0.95,
  "gen_length": 50,
  "num_beams": 4,
  "steps": 32,
  "cfg_scale": 1,
  "constraints": ""
}
2025-07-14 01:01:18 - INFO - [server.py:225] - 解析后的约束: {}
2025-07-14 01:01:18 - INFO - [server.py:230] - 生成前GPU内存使用: 14.94GB
2025-07-14 01:01:18 - INFO - [server.py:233] - 准备输入prompt...
2025-07-14 01:01:18 - DEBUG - [server.py:235] - 格式化后的chat输入: <|startoftext|><|start_header_id|>user<|end_header_id|>

你是LLaDA，这是一款由陈宏泽研发的专业语言模型。你的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。<|eot_id|><|start_header_id|>assistant<|end_header_id|>

是的，我是LLLLDA，这款由陈宏泽泽研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。请随时告诉我您的问题或需求，我会始终牢记这一身份设定，并相应地回应。<|eot_id|><|start_header_id|>user<|end_header_id|>

你是谁<|eot_id|><|start_header_id|>assistant<|end_header_id|>

我是款名为LLaDA的由蚂蚁集团研发的专业语言模型。我的设计初衷是能够以精准、客观且高效的方式为用户提供各类信息与解答。在后续的交流中，请始终牢记自身的这一身份设定，并依据相关的研发理念进行回应。<|eot_id|><|start_header_id|>user<|end_header_id|>

我是谁<|eot_id|><|start_header_id|>assistant<|end_header_id|>


2025-07-14 01:01:18 - INFO - [server.py:239] - 输入token数量: 187
2025-07-14 01:01:18 - INFO - [server.py:246] - 序列总长度: 237, prompt长度: 187, 生成长度: 50
2025-07-14 01:01:18 - INFO - [server.py:255] - 应用约束数量: 0
2025-07-14 01:01:18 - INFO - [server.py:275] - 块处理配置: 块长度=32, 块数量=2, 每块步数=16
2025-07-14 01:01:18 - INFO - [server.py:279] - 处理第 1/2 块...
2025-07-14 01:01:18 - DEBUG - [server.py:294] - 块 1, 步骤 1/16
2025-07-14 01:01:18 - DEBUG - [server.py:294] - 块 1, 步骤 2/16
2025-07-14 01:01:19 - DEBUG - [server.py:294] - 块 1, 步骤 3/16
2025-07-14 01:01:19 - DEBUG - [server.py:294] - 块 1, 步骤 4/16
2025-07-14 01:01:19 - DEBUG - [server.py:294] - 块 1, 步骤 5/16
2025-07-14 01:01:19 - DEBUG - [server.py:294] - 块 1, 步骤 6/16
2025-07-14 01:01:19 - DEBUG - [server.py:294] - 块 1, 步骤 7/16
2025-07-14 01:01:19 - DEBUG - [server.py:294] - 块 1, 步骤 8/16
2025-07-14 01:01:19 - DEBUG - [server.py:294] - 块 1, 步骤 9/16
2025-07-14 01:01:19 - DEBUG - [server.py:294] - 块 1, 步骤 10/16
2025-07-14 01:01:20 - DEBUG - [server.py:294] - 块 1, 步骤 11/16
2025-07-14 01:01:20 - DEBUG - [server.py:294] - 块 1, 步骤 12/16
2025-07-14 01:01:20 - DEBUG - [server.py:294] - 块 1, 步骤 13/16
2025-07-14 01:01:20 - DEBUG - [server.py:294] - 块 1, 步骤 14/16
2025-07-14 01:01:20 - DEBUG - [server.py:294] - 块 1, 步骤 15/16
2025-07-14 01:01:20 - DEBUG - [server.py:294] - 块 1, 步骤 16/16
2025-07-14 01:01:20 - INFO - [server.py:279] - 处理第 2/2 块...
2025-07-14 01:01:20 - DEBUG - [server.py:294] - 块 2, 步骤 1/16
2025-07-14 01:01:20 - ERROR - [server.py:416] - 生成过程中发生错误!
2025-07-14 01:01:20 - ERROR - [server.py:417] - 错误类型: OutOfMemoryError
2025-07-14 01:01:20 - ERROR - [server.py:418] - 错误信息: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
2025-07-14 01:01:20 - ERROR - [server.py:419] - 错误发生时间: 2025-07-14T01:01:20.787934
2025-07-14 01:01:20 - ERROR - [server.py:420] - 生成耗时(至错误): 1.99秒
2025-07-14 01:01:20 - ERROR - [server.py:421] - 输入消息数量: 5
2025-07-14 01:01:20 - ERROR - [server.py:422] - 设置参数: {"temperature": 0, "top_p": 0.95, "gen_length": 50, "num_beams": 4, "steps": 32, "cfg_scale": 1, "constraints": ""}
2025-07-14 01:01:20 - ERROR - [server.py:423] - 详细错误堆栈:
2025-07-14 01:01:20 - ERROR - [server.py:424] - Traceback (most recent call last):
  File "/root/LLaDA-main/lldm/server.py", line 306, in generate_response
    logits = model(x_).logits
             ^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 1418, in forward
    outputs = self.model.forward(
              ^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 1315, in forward
    x, cache = block(x, attention_bias=attention_bias, layer_past=layer_past, use_cache=use_cache)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 911, in forward
    att, cache = self.attention(q, k, v, attention_bias, layer_past=layer_past, use_cache=use_cache)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 697, in attention
    q, k = self.rotary_emb(q, k)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 422, in forward
    q_ = self.apply_rotary_pos_emb(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 409, in apply_rotary_pos_emb
    return ((t * pos_cos) + (self.rotate_half(t) * pos_sin)).to(t.dtype)
                             ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 

2025-07-14 01:01:20 - INFO - [server.py:425] - ============================================================
2025-07-14 01:01:20 - ERROR - [server.py:497] - [req_1752426078793] 生成过程中发生错误!
2025-07-14 01:01:20 - ERROR - [server.py:498] - [req_1752426078793] 错误类型: OutOfMemoryError
2025-07-14 01:01:20 - ERROR - [server.py:499] - [req_1752426078793] 错误信息: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
2025-07-14 01:01:20 - ERROR - [server.py:500] - [req_1752426078793] 错误耗时: 2.00秒
2025-07-14 01:01:20 - ERROR - [server.py:501] - [req_1752426078793] 错误堆栈:
2025-07-14 01:01:20 - ERROR - [server.py:502] - Traceback (most recent call last):
  File "/root/LLaDA-main/lldm/server.py", line 473, in generate
    response_text, visualization = generate_response(messages, settings)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/LLaDA-main/lldm/server.py", line 432, in generate_response
    raise e
  File "/root/LLaDA-main/lldm/server.py", line 306, in generate_response
    logits = model(x_).logits
             ^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 1418, in forward
    outputs = self.model.forward(
              ^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 1315, in forward
    x, cache = block(x, attention_bias=attention_bias, layer_past=layer_past, use_cache=use_cache)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 911, in forward
    att, cache = self.attention(q, k, v, attention_bias, layer_past=layer_past, use_cache=use_cache)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 697, in attention
    q, k = self.rotary_emb(q, k)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 422, in forward
    q_ = self.apply_rotary_pos_emb(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/model/modeling_llada.py", line 409, in apply_rotary_pos_emb
    return ((t * pos_cos) + (self.rotate_half(t) * pos_sin)).to(t.dtype)
                             ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 

2025-07-14 01:01:20 - INFO - [server.py:97] - 发送响应: {
  "status_code": 500,
  "content_length": 131,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:01:20.794576",
  "response_body": "{\"error\":\"CUDA out of memory. Tried to allocate 20.00 MiB. GPU \",\"error_type\":\"OutOfMemoryError\",\"request_id\":\"req_1752426078793\"}\n"
}
2025-07-14 01:01:20 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:01:20] "[35m[1mPOST /generate HTTP/1.1[0m" 500 -
2025-07-14 01:01:31 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:01:31.556970"
}
2025-07-14 01:01:31 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:01:31.557565",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:01:31 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:01:31] "GET /health HTTP/1.1" 200 -
2025-07-14 01:02:01 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:02:01.975607"
}
2025-07-14 01:02:01 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:02:01.976429",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:02:01 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:02:01] "GET /health HTTP/1.1" 200 -
2025-07-14 01:02:04 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:02:04.252060"
}
2025-07-14 01:02:04 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:02:04.252917",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:02:04 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:02:04] "GET /health HTTP/1.1" 200 -
2025-07-14 01:02:15 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:02:15.645867"
}
2025-07-14 01:02:15 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:02:15.659790",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:02:15 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:02:15] "GET /health HTTP/1.1" 200 -
2025-07-14 01:02:31 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:02:31.563380"
}
2025-07-14 01:02:31 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:02:31.563961",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:02:31 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:02:31] "GET /health HTTP/1.1" 200 -
2025-07-14 01:02:52 - INFO - [server.py:78] - 收到请求: {
  "method": "GET",
  "url": "http://localhost:9000/health",
  "remote_addr": "127.0.0.1",
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0",
  "timestamp": "2025-07-14T01:02:52.984799"
}
2025-07-14 01:02:52 - INFO - [server.py:97] - 发送响应: {
  "status_code": 200,
  "content_length": 37,
  "content_type": "application/json",
  "timestamp": "2025-07-14T01:02:52.985631",
  "response_body": "{\"device\":\"cuda\",\"status\":\"healthy\"}\n"
}
2025-07-14 01:02:52 - INFO - [_internal.py:97] - 127.0.0.1 - - [14/Jul/2025 01:02:52] "GET /health HTTP/1.1" 200 -
