=== LLaDA åç«¯æœåŠ¡æ—¥å¿— ===
å¯åŠ¨æ—¶é—´: Mon Jul 14 00:43:24 CST 2025
æ—¥å¿—ä¼šè¯: 20250714_004324
Pythonç‰ˆæœ¬: Python 3.12.3
å·¥ä½œç›®å½•: /root/LLaDA-main/lldm
è®¾å¤‡æ£€æµ‹: CUDAå¯ç”¨: True
==========================

Using device: cuda
GPU memory before loading: 23.5GB
Loading tokenizer...
Loading model...
The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:00<00:02,  1.89it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:01<00:02,  1.72it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:01<00:01,  1.57it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:02<00:01,  1.52it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:03<00:00,  1.44it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.60it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.58it/s]
Model loaded successfully on cuda
GPU memory after loading: 14.9GB allocated
 * Serving Flask app 'server'
 * Debug mode: off
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://172.17.0.3:5000
[33mPress CTRL+C to quit[0m
