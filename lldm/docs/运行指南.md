# LLaDA 模型对话系统运行指南

## 📋 运行前检查

### 1. 系统要求
- ✅ Python 3.7+
- ✅ Node.js 14+
- ✅ npm 包管理器
- ✅ CUDA（推荐，用于GPU加速）

### 2. 模型文件检查
确保LLaDA模型文件在正确位置：
```bash
ls -la /root/autodl-tmp/model/
```
应该看到模型文件（如 `config.json`, `pytorch_model.bin` 等）

### 3. 依赖安装检查
```bash
# Python依赖
pip3 list | grep -E "(flask|torch|transformers)"

# Node.js依赖
npm list --depth=0
```

## 🚀 启动方式

### 方式1：一键启动（推荐）
```bash
cd /root/LLaDA-main/lldm
chmod +x start.sh
./start.sh
```

### 方式2：分步启动
```bash
# 终端1 - 启动后端
cd /root/LLaDA-main/lldm
python3 server.py

# 终端2 - 启动前端
cd /root/LLaDA-main/lldm
npm start
```

### 方式3：测试启动
```bash
cd /root/LLaDA-main/lldm
chmod +x quick_start.sh
./quick_start.sh
```

## 🔧 访问地址

启动成功后，访问以下地址：
- **前端界面**: http://localhost:3000
- **后端API**: http://localhost:9000
- **健康检查**: http://localhost:9000/health

## 📊 系统流程

详细的系统工作流程请参考：[系统流程图](系统流程图.md)

包含以下流程图：
- 🏗️ **整体架构流程图** - 完整的组件交互关系
- 🔄 **数据流转图** - 从输入到输出的详细时序
- 🧠 **核心算法流程图** - 扩散生成的内部逻辑
- 🎨 **置信度颜色编码** - 可视化处理流程
- ⚠️ **错误处理流程** - 异常情况的处理机制

## 📱 使用说明

### 基本对话
1. 在输入框输入消息
2. 点击"发送"按钮
3. 观察扩散生成过程

### 高级功能
- **约束生成**: 在约束输入框输入 `0:你好, 5:世界`
- **参数调节**: 调整滑动条改变生成参数
- **可视化**: 点击"显示指示器"查看置信度说明

## 🐛 常见问题

### 1. 后端启动失败
```bash
# 检查模型路径
ls /root/autodl-tmp/model/

# 检查Python依赖
pip3 install flask flask-cors torch transformers

# 检查端口占用
netstat -tlnp | grep :9000
```

### 2. 前端启动失败
```bash
# 重新安装依赖
rm -rf node_modules package-lock.json
npm install

# 检查端口占用
netstat -tlnp | grep :3000
```

### 3. 连接失败
```bash
# 测试后端API
curl http://localhost:9000/health

# 检查CORS设置
curl -H "Origin: http://localhost:3000" http://localhost:9000/health
```

## 📋 日志系统

### 日志目录结构
系统使用会话分组的日志管理：
```
logs/
├── session_20250713_143052/  # 当前会话
│   ├── backend.log           # 后端详细日志
│   ├── frontend.log          # 前端构建日志  
│   └── system.log           # 系统操作日志
├── backend.log -> session_xxx/backend.log  # 软链接
└── frontend.log -> session_xxx/frontend.log
```

### 日志监控命令
```bash
# 实时查看所有日志
tail -f logs/session_*/backend.log logs/session_*/frontend.log

# 查看最新会话日志
tail -f backend.log frontend.log

# 错误过滤
grep -i error logs/session_*/*.log

# 特定请求追踪（使用请求ID）
grep "req_123456" logs/session_*/backend.log
```

### 日志内容说明
- **后端日志**: 请求参数、消息内容、GPU内存使用、模型推理详情、错误堆栈
- **前端日志**: API调用记录、用户交互、组件状态变化、性能指标
- **系统日志**: 启动流程、环境检测、服务状态、错误摘要

所有日志包含时间戳和请求ID，便于问题追踪和分析。

## 🧪 测试验证

### 测试后端API
```bash
python3 test_api.py
```

### 测试前端功能
1. 访问 http://localhost:3000
2. 输入："你好，请介绍一下你自己"
3. 观察生成过程
4. 尝试约束生成
5. 调整参数测试

## 📊 性能优化

### GPU加速
确保CUDA可用：
```bash
python3 -c "import torch; print(torch.cuda.is_available())"
```

### 内存优化
- 调整生成长度（推荐64以下）
- 减少去噪步数（推荐16-32）
- 关闭不必要的程序释放内存

## 🔄 重启系统

如果遇到问题需要重启：
```bash
# 停止所有相关进程
pkill -f "python3 server.py"
pkill -f "npm start"

# 重新启动
./start.sh
```

## 📝 开发调试

### 查看日志
```bash
# 后端日志（新版会话日志）
tail -f backend.log
# 或查看具体会话
tail -f logs/session_20250713_143052/backend.log

# 前端日志
tail -f frontend.log

# 系统日志
tail -f logs/session_*/system.log

# 同时监控所有日志
tail -f logs/session_*/*.log

# 错误日志过滤
grep -i "error\|exception\|failed" logs/session_*/*.log
```

### 修改配置
- 后端配置: `server.py`
- 前端配置: `src/DiffusionModel.js`
- 样式配置: `src/DiffusionModel.css`

## 🎯 快速验证

运行以下命令进行快速验证：
```bash
# 1. 检查环境
python3 --version
npm --version

# 2. 检查模型
ls /root/autodl-tmp/model/

# 3. 测试后端
cd /root/LLaDA-main/lldm && python3 test_api.py

# 4. 启动系统
./start.sh
```

## ✅ 成功标志

系统启动成功的标志：
- ✅ 后端显示："Using device: cuda/cpu"
- ✅ 前端显示："webpack compiled successfully"
- ✅ 健康检查返回：`{"status": "healthy"}`
- ✅ 浏览器可以正常访问界面

如果遇到问题，请检查上述步骤或查看错误日志。
