from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import re
import logging
import json
import traceback
from datetime import datetime
import sys
import os

# é…ç½®è¯¦ç»†çš„æ—¥å¿—è®°å½•
def setup_logging():
    """è®¾ç½®è¯¦ç»†çš„æ—¥å¿—è®°å½•"""
    # è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
    import os
    log_file = os.environ.get('BACKEND_LOG_FILE', 'backend.log')
    
    # åˆ›å»ºæ—¥å¿—æ ¼å¼
    log_format = logging.Formatter(
        '%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # è®¾ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    
    # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # æ–‡ä»¶å¤„ç†å™¨ - è¯¦ç»†æ—¥å¿—
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(log_format)
    root_logger.addHandler(file_handler)
    
    # æ§åˆ¶å°å¤„ç†å™¨ - ç®€è¦ä¿¡æ¯
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(log_format)
    root_logger.addHandler(console_handler)
    
    return root_logger

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logging()

app = Flask(__name__)
CORS(app)  # å¯ç”¨CORSä»¥æ”¯æŒå‰ç«¯è°ƒç”¨

# è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
@app.before_request
def log_request():
    """è®°å½•æ‰€æœ‰è¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯"""
    request_data = {
        'method': request.method,
        'url': request.url,
        'remote_addr': request.remote_addr,
        'user_agent': str(request.user_agent),
        'timestamp': datetime.now().isoformat()
    }
    
    # è®°å½•è¯·æ±‚ä½“ï¼ˆå¦‚æœæœ‰ï¼‰
    if request.is_json:
        try:
            request_data['json_data'] = request.get_json()
        except Exception as e:
            request_data['json_error'] = str(e)
    elif request.form:
        request_data['form_data'] = dict(request.form)
    elif request.args:
        request_data['args'] = dict(request.args)
    
    logger.info(f"æ”¶åˆ°è¯·æ±‚: {json.dumps(request_data, ensure_ascii=False, indent=2)}")

@app.after_request
def log_response(response):
    """è®°å½•æ‰€æœ‰å“åº”çš„è¯¦ç»†ä¿¡æ¯"""
    response_data = {
        'status_code': response.status_code,
        'content_length': response.content_length,
        'content_type': response.content_type,
        'timestamp': datetime.now().isoformat()
    }
    
    # è®°å½•å“åº”ä½“ï¼ˆå°äº1000å­—ç¬¦æ—¶ï¼‰
    if response.content_length and response.content_length < 1000:
        try:
            response_data['response_body'] = response.get_data(as_text=True)
        except Exception as e:
            response_data['response_error'] = str(e)
    
    logger.info(f"å‘é€å“åº”: {json.dumps(response_data, ensure_ascii=False, indent=2)}")
    return response

# å…¨å±€æ¨¡å‹å’Œtokenizer
device = 'cuda' if torch.cuda.is_available() else 'cpu'
logger.info(f"åˆå§‹åŒ–è®¾å¤‡: {device}")

try:
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"GPUæ€»å†…å­˜: {gpu_memory_total:.1f}GB")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    logger.info("å¼€å§‹åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/model', trust_remote_code=True)
    logger.info("TokenizeråŠ è½½æˆåŠŸ")
    
    logger.info("å¼€å§‹åŠ è½½æ¨¡å‹...")
    try:
        # å°è¯•ä»¥åŠç²¾åº¦åŠ è½½åˆ°GPU
        if device == 'cuda':
            model = AutoModel.from_pretrained('/root/autodl-tmp/model', trust_remote_code=True, 
                                            torch_dtype=torch.bfloat16, device_map="auto")
        else:
            model = AutoModel.from_pretrained('/root/autodl-tmp/model', trust_remote_code=True)
    except torch.cuda.OutOfMemoryError as e:
        logger.error(f"GPUå†…å­˜ä¸è¶³: {e}")
        logger.info("å›é€€åˆ°CPU...")
        device = 'cpu'
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
        # åœ¨CPUä¸ŠåŠ è½½æ¨¡å‹
        model = AutoModel.from_pretrained('/root/autodl-tmp/model', trust_remote_code=True, 
                                        torch_dtype=torch.float32).to(device)
    
    logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    if device == 'cuda':
        gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
        logger.info(f"GPUå·²åˆ†é…å†…å­˜: {gpu_memory_allocated:.1f}GB")
        
except Exception as e:
    logger.error(f"æ¨¡å‹åŠ è½½é”™è¯¯: {e}")
    logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    logger.info("å›é€€åˆ°CPU...")
    device = 'cpu'
    model = AutoModel.from_pretrained('/root/autodl-tmp/model', trust_remote_code=True).to(device)

# å¸¸é‡
MASK_TOKEN = "[MASK]"
MASK_ID = 126336

def parse_constraints(constraints_text):
    """è§£æçº¦æŸæ¡ä»¶"""
    constraints = {}
    if not constraints_text:
        return constraints
        
    parts = constraints_text.split(',')
    for part in parts:
        if ':' not in part:
            continue
        pos_str, word = part.split(':', 1)
        try:
            pos = int(pos_str.strip())
            word = word.strip()
            if word and pos >= 0:
                constraints[pos] = word
        except ValueError:
            continue
    
    return constraints

def format_chat_history(history):
    """æ ¼å¼åŒ–èŠå¤©å†å²"""
    messages = []
    for msg in history:
        messages.append({"role": msg["role"], "content": msg["content"]})
    return messages

def add_gumbel_noise(logits, temperature):
    """æ·»åŠ Gumbelå™ªå£°"""
    if temperature <= 0:
        return logits
        
    logits = logits.to(torch.float64)
    noise = torch.rand_like(logits, dtype=torch.float64)
    gumbel_noise = (- torch.log(noise)) ** temperature
    return logits.exp() / gumbel_noise

def get_num_transfer_tokens(mask_index, steps):
    """è®¡ç®—è½¬ç§»tokenæ•°é‡"""
    mask_num = mask_index.sum(dim=1, keepdim=True)
    base = mask_num // steps
    remainder = mask_num % steps
    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base
    for i in range(mask_num.size(0)):
        num_transfer_tokens[i, :remainder[i]] += 1
    return num_transfer_tokens

def generate_response(messages, settings):
    """
    ç”Ÿæˆå“åº”ä¸»å‡½æ•°
    è¿”å›: (response_text, visualization_steps)
    """
    start_time = datetime.now()
    
    # è®°å½•è¯¦ç»†çš„è¾“å…¥å‚æ•°
    logger.info("=" * 60)
    logger.info("å¼€å§‹ç”Ÿæˆå“åº”")
    logger.info(f"è¾“å…¥æ¶ˆæ¯æ•°é‡: {len(messages)}")
    for i, msg in enumerate(messages):
        logger.info(f"æ¶ˆæ¯ {i+1}: è§’è‰²={msg.get('role', 'unknown')}, å†…å®¹é•¿åº¦={len(msg.get('content', ''))}")
        logger.debug(f"æ¶ˆæ¯ {i+1} å†…å®¹: {msg.get('content', '')}")
    
    logger.info(f"è®¾ç½®å‚æ•°: {json.dumps(settings, ensure_ascii=False, indent=2)}")
    
    try:
        # è§£æè®¾ç½®å‚æ•°
        gen_length = settings.get("gen_length", 64)
        steps = settings.get("steps", 32)
        constraints = parse_constraints(settings.get("constraints", ""))
        temperature = settings.get("temperature", 0.0)
        cfg_scale = settings.get("cfg_scale", 0.0)
        block_length = settings.get("block_length", 32)
        remasking = settings.get("remasking", "low_confidence")
        
        logger.info(f"è§£æåçš„çº¦æŸ: {constraints}")
        
        # è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
        if device == 'cuda':
            memory_before = torch.cuda.memory_allocated() / 1024**3
            logger.info(f"ç”Ÿæˆå‰GPUå†…å­˜ä½¿ç”¨: {memory_before:.2f}GB")
        
        # å‡†å¤‡prompt
        logger.info("å‡†å¤‡è¾“å…¥prompt...")
        chat_input = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        logger.debug(f"æ ¼å¼åŒ–åçš„chatè¾“å…¥: {chat_input}")
        
        input_ids = tokenizer(chat_input)['input_ids']
        input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)
        logger.info(f"è¾“å…¥tokenæ•°é‡: {len(input_ids[0])}")
        
        # åˆå§‹åŒ–åºåˆ—
        prompt_length = input_ids.shape[1]
        x = torch.full((1, prompt_length + gen_length), MASK_ID, dtype=torch.long).to(device)
        x[:, :prompt_length] = input_ids.clone()
        
        logger.info(f"åºåˆ—æ€»é•¿åº¦: {x.shape[1]}, prompté•¿åº¦: {prompt_length}, ç”Ÿæˆé•¿åº¦: {gen_length}")
        
        # åº”ç”¨çº¦æŸ
        processed_constraints = {}
        for pos, word in constraints.items():
            tokens = tokenizer.encode(" " + word, add_special_tokens=False)
            for i, token_id in enumerate(tokens):
                processed_constraints[pos + i] = token_id
        
        logger.info(f"åº”ç”¨çº¦æŸæ•°é‡: {len(processed_constraints)}")
        for pos, token_id in processed_constraints.items():
            absolute_pos = prompt_length + pos
            if absolute_pos < x.shape[1]:
                x[:, absolute_pos] = token_id
                logger.debug(f"çº¦æŸä½ç½® {pos} (ç»å¯¹ä½ç½® {absolute_pos}): token_id={token_id}")
        
        # åˆå§‹åŒ–å¯è§†åŒ–çŠ¶æ€
        visualization_steps = []
        initial_state = [(MASK_TOKEN, "#444444") for _ in range(gen_length)]
        visualization_steps.append(initial_state)
        
        # æ ‡è®°promptä½ç½®
        prompt_index = (x != MASK_ID)
        
        # å¤„ç†block
        block_length = min(block_length, gen_length)
        num_blocks = (gen_length + block_length - 1) // block_length
        steps_per_block = max(1, steps // num_blocks)
        
        logger.info(f"å—å¤„ç†é…ç½®: å—é•¿åº¦={block_length}, å—æ•°é‡={num_blocks}, æ¯å—æ­¥æ•°={steps_per_block}")
        
        # å¤„ç†æ¯ä¸ªblock
        for block_idx in range(num_blocks):
            logger.info(f"å¤„ç†ç¬¬ {block_idx + 1}/{num_blocks} å—...")
            
            block_start = prompt_length + block_idx * block_length
            block_end = min(prompt_length + (block_idx + 1) * block_length, x.shape[1])
            block_mask_index = (x[:, block_start:block_end] == MASK_ID)
            
            if not block_mask_index.any():
                logger.info(f"å— {block_idx + 1} æ²¡æœ‰MASK tokenï¼Œè·³è¿‡")
                continue
            
            # è®¡ç®—å½“å‰å—çš„è½¬ç§»tokenæ•°é‡
            num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)
            
            # å¤„ç†æ¯ä¸ªstep
            for step_idx in range(steps_per_block):
                logger.debug(f"å— {block_idx + 1}, æ­¥éª¤ {step_idx + 1}/{steps_per_block}")
                
                mask_index = (x == MASK_ID)
                if not mask_index.any():
                    logger.debug("æ²¡æœ‰å‰©ä½™çš„MASK tokenï¼Œæå‰ç»“æŸ")
                    break
                    
                # åº”ç”¨åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼
                if cfg_scale > 0.0:
                    un_x = x.clone()
                    un_x[prompt_index] = MASK_ID
                    x_ = torch.cat([x, un_x], dim=0)
                    logits = model(x_).logits
                    logits, un_logits = torch.chunk(logits, 2, dim=0)
                    logits = un_logits + (cfg_scale + 1) * (logits - un_logits)
                else:
                    logits = model(x).logits
                
                # æ·»åŠ å™ªå£°
                logits_with_noise = add_gumbel_noise(logits, temperature)
                x0 = torch.argmax(logits_with_noise, dim=-1)
                
                # è®¡ç®—ç½®ä¿¡åº¦
                if remasking == 'low_confidence':
                    p = F.softmax(logits.to(torch.float64), dim=-1)
                    x0_p = torch.squeeze(torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1)
                elif remasking == 'random':
                    x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)
                else:
                    raise ValueError(f"Unknown remasking strategy: {remasking}")
                
                # ä¸è€ƒè™‘å½“å‰å—ä¹‹å¤–çš„ä½ç½®
                x0_p[:, block_end:] = -float('inf')
                
                # æ›´æ–°token
                old_x = x.clone()
                x0 = torch.where(mask_index, x0, x)
                confidence = torch.where(mask_index, x0_p, -float('inf'))
                
                # é€‰æ‹©è¦è½¬ç§»çš„token
                transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)
                for j in range(confidence.shape[0]):
                    # åªè€ƒè™‘å½“å‰å—çš„ç½®ä¿¡åº¦
                    block_confidence = confidence[j, block_start:block_end]
                    if step_idx < steps_per_block - 1:  # ä¸æ˜¯æœ€åä¸€æ­¥
                        # é€‰æ‹©top-kç½®ä¿¡åº¦æœ€é«˜çš„token
                        _, select_indices = torch.topk(
                            block_confidence, 
                            k=min(num_transfer_tokens[j, step_idx].item(), block_confidence.numel())
                        )
                        # è°ƒæ•´ç´¢å¼•åˆ°å…¨å±€ä½ç½®
                        select_indices = select_indices + block_start
                        transfer_index[j, select_indices] = True
                    else:  # æœ€åä¸€æ­¥ - å–æ¶ˆæ‰€æœ‰å‰©ä½™çš„mask
                        transfer_index[j, block_start:block_end] = mask_index[j, block_start:block_end]
                
                # åº”ç”¨æ›´æ–°
                x = torch.where(transfer_index, x0, x)
                
                # ç¡®ä¿çº¦æŸ
                for pos, token_id in processed_constraints.items():
                    absolute_pos = prompt_length + pos
                    if absolute_pos < x.shape[1]:
                        x[:, absolute_pos] = token_id
                
                # åˆ›å»ºå¯è§†åŒ–çŠ¶æ€
                current_state = []
                for i in range(gen_length):
                    pos = prompt_length + i
                    
                    if x[0, pos] == MASK_ID:
                        current_state.append((MASK_TOKEN, "#444444"))
                    elif old_x[0, pos] == MASK_ID:
                        token = tokenizer.decode([x[0, pos].item()], skip_special_tokens=False)
                        conf = float(x0_p[0, pos].cpu())
                        if conf < 0.3:
                            color = "#FF6666"
                        elif conf < 0.7:
                            color = "#FFAA33"
                        else:
                            color = "#66CC66"
                        current_state.append((token, color))
                    else:
                        token = tokenizer.decode([x[0, pos].item()], skip_special_tokens=False)
                        current_state.append((token, "#6699CC"))
                
                visualization_steps.append(current_state)
                
                # æ¸…ç†GPUå†…å­˜
                if device == 'cuda':
                    torch.cuda.empty_cache()
    
        # æå–æœ€ç»ˆå“åº”
        response_tokens = x[0, prompt_length:]
        final_text = tokenizer.decode(
            response_tokens, 
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        # è®°å½•ç”Ÿæˆç»“æœ
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        if device == 'cuda':
            memory_after = torch.cuda.memory_allocated() / 1024**3
            logger.info(f"ç”ŸæˆåGPUå†…å­˜ä½¿ç”¨: {memory_after:.2f}GB")
        
        logger.info(f"ç”Ÿæˆå®Œæˆ!")
        logger.info(f"å“åº”æ–‡æœ¬é•¿åº¦: {len(final_text)}")
        logger.info(f"å¯è§†åŒ–æ­¥éª¤æ•°: {len(visualization_steps)}")
        logger.info(f"ç”Ÿæˆè€—æ—¶: {duration:.2f}ç§’")
        logger.debug(f"ç”Ÿæˆçš„å“åº”æ–‡æœ¬: {final_text}")
        logger.info("=" * 60)
        
        return final_text, visualization_steps
        
    except Exception as e:
        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        error_time = datetime.now()
        duration = (error_time - start_time).total_seconds()
        
        logger.error("ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯!")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        logger.error(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        logger.error(f"é”™è¯¯å‘ç”Ÿæ—¶é—´: {error_time.isoformat()}")
        logger.error(f"ç”Ÿæˆè€—æ—¶(è‡³é”™è¯¯): {duration:.2f}ç§’")
        logger.error(f"è¾“å…¥æ¶ˆæ¯æ•°é‡: {len(messages) if 'messages' in locals() else 'æœªçŸ¥'}")
        logger.error(f"è®¾ç½®å‚æ•°: {json.dumps(settings, ensure_ascii=False) if 'settings' in locals() else 'æœªçŸ¥'}")
        logger.error(f"è¯¦ç»†é”™è¯¯å †æ ˆ:")
        logger.error(traceback.format_exc())
        logger.info("=" * 60)
        
        # æ¸…ç†GPUå†…å­˜
        if device == 'cuda':
            torch.cuda.empty_cache()
            
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸
        raise e


@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({"status": "healthy", "device": device})

@app.route('/generate', methods=['POST'])
def generate():
    """ç”ŸæˆAPIç«¯ç‚¹"""
    request_time = datetime.now()
    request_id = f"req_{int(request_time.timestamp() * 1000)}"
    
    try:
        # è®°å½•è¯·æ±‚å¼€å§‹
        logger.info(f"[{request_id}] æ”¶åˆ°ç”Ÿæˆè¯·æ±‚")
        
        # è·å–è¯·æ±‚ä½“
        data = request.json
        messages = data.get('messages', [])
        settings = data.get('settings', {})
        
        # è¯¦ç»†è®°å½•è¯·æ±‚æ•°æ®
        logger.info(f"[{request_id}] è¯·æ±‚æ•°æ®è¯¦æƒ…:")
        logger.info(f"[{request_id}] - æ¶ˆæ¯æ•°é‡: {len(messages)}")
        logger.info(f"[{request_id}] - è®¾ç½®å‚æ•°: {json.dumps(settings, ensure_ascii=False, indent=2)}")
        logger.debug(f"[{request_id}] - å®Œæ•´æ¶ˆæ¯åˆ—è¡¨: {json.dumps(messages, ensure_ascii=False, indent=2)}")
        
        # éªŒè¯è¾“å…¥
        if not messages:
            logger.warning(f"[{request_id}] ç©ºæ¶ˆæ¯åˆ—è¡¨")
            return jsonify({"error": "Empty message list"}), 400
            
        if messages[-1]['role'] != 'user':
            logger.warning(f"[{request_id}] æœ€åä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯: {messages[-1].get('role', 'unknown')}")
            return jsonify({"error": "Last message must be from user"}), 400
        
        logger.info(f"[{request_id}] å¼€å§‹ç”Ÿæˆå“åº”...")
        
        # ç”Ÿæˆå“åº”
        response_text, visualization = generate_response(messages, settings)
        
        # è®°å½•æˆåŠŸå“åº”
        response_time = datetime.now()
        duration = (response_time - request_time).total_seconds()
        
        logger.info(f"[{request_id}] ç”ŸæˆæˆåŠŸ!")
        logger.info(f"[{request_id}] å“åº”é•¿åº¦: {len(response_text)}")
        logger.info(f"[{request_id}] å¯è§†åŒ–æ­¥éª¤: {len(visualization)}")
        logger.info(f"[{request_id}] æ€»è€—æ—¶: {duration:.2f}ç§’")
        logger.debug(f"[{request_id}] å“åº”æ–‡æœ¬: {response_text}")
        
        return jsonify({
            "response": response_text,
            "visualization": visualization,
            "request_id": request_id,
            "duration": duration
        })
        
    except Exception as e:
        # è®°å½•é”™è¯¯
        error_time = datetime.now()
        duration = (error_time - request_time).total_seconds()
        
        logger.error(f"[{request_id}] ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯!")
        logger.error(f"[{request_id}] é”™è¯¯ç±»å‹: {type(e).__name__}")
        logger.error(f"[{request_id}] é”™è¯¯ä¿¡æ¯: {str(e)}")
        logger.error(f"[{request_id}] é”™è¯¯è€—æ—¶: {duration:.2f}ç§’")
        logger.error(f"[{request_id}] é”™è¯¯å †æ ˆ:")
        logger.error(traceback.format_exc())
        
        return jsonify({
            "error": str(e), 
            "request_id": request_id,
            "error_type": type(e).__name__
        }), 500

if __name__ == '__main__':
    # è®°å½•å¯åŠ¨ä¿¡æ¯
    logger.info("=" * 50)
    logger.info("ğŸš€ LLaDAåç«¯æœåŠ¡å¯åŠ¨")
    logger.info(f"å¯åŠ¨æ—¶é—´: {datetime.now().isoformat()}")
    logger.info(f"Pythonç‰ˆæœ¬: {sys.version}")
    logger.info(f"å·¥ä½œç›®å½•: {os.getcwd()}")
    logger.info(f"è®¾å¤‡: {device}")
    if device == 'cuda':
        logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    logger.info("=" * 50)
    
    app.run(host='0.0.0.0', port=9000, threaded=True)